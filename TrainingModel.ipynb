{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('tkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import keras\n",
    "import glob\n",
    "import os\n",
    "from time import time\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from keras.callbacks import TensorBoard\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "logsdir = os.path.join(cwd,\"logs10\")\n",
    "batchsize = 8\n",
    "number_class = 7\n",
    "epochs = 10\n",
    "ratio_testset = 0.15\n",
    "lrate = 0.0001\n",
    "numbatchsplit = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 10%..\n",
      "Loading 30%..\n",
      "Loading 50%..\n",
      "Loading 70%..\n",
      "Loading 90%..\n",
      "Finish\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#------------First time------------\n",
    "y1 = len(glob.glob(os.path.join(\"Dataset\\\\Image100_resize\" ,'*jpg')))\n",
    "y2 = len(glob.glob(os.path.join(\"Dataset\\\\Image110_resize\" ,'*jpg')))\n",
    "y3 = len(glob.glob(os.path.join(\"Dataset\\\\Image120_resize\" ,'*jpg')))\n",
    "y4 = len(glob.glob(os.path.join(\"Dataset\\\\Image130_resize\" ,'*jpg')))\n",
    "y5 = len(glob.glob(os.path.join(\"Dataset\\\\Image140_resize\" ,'*jpg')))\n",
    "y6 = len(glob.glob(os.path.join(\"Dataset\\\\Image150_resize\" ,'*jpg')))\n",
    "y7 = len(glob.glob(os.path.join(\"Dataset\\\\Image160_resize\" ,'*jpg')))\n",
    "\n",
    "# X_data\n",
    "X = []\n",
    "count = 0\n",
    "for i in glob.glob(os.path.join(\"Dataset\\\\Image100_resize\" ,'*jpg')): #100\n",
    "\timg = cv2.imread(i)\t\n",
    "\tif count == 0:\n",
    "\t\tX = img \n",
    "\t\tcount = 1\n",
    "\telse:\n",
    "\t\tX = np.append(X,img, axis = 0)\t\n",
    "print(\"Loading 10%..\")\n",
    "for i in glob.glob(os.path.join(\"Dataset\\\\Image110_resize\" ,'*jpg')): #110\n",
    "\timg = cv2.imread(i)\t\n",
    "\t#img = cv2.resize(img, (66,200), interpolation = cv2.INTER_AREA)\n",
    "\tX = np.append(X,img, axis = 0)\n",
    "print(\"Loading 30%..\")\n",
    "for i in glob.glob(os.path.join(\"Dataset\\\\Image120_resize\" ,'*jpg')): #120\n",
    "\timg = cv2.imread(i)\t\n",
    "\t#img = cv2.resize(img, (66,200), interpolation = cv2.INTER_AREA)\n",
    "\tX = np.append(X,img, axis = 0)\n",
    "print(\"Loading 50%..\")\n",
    "for i in glob.glob(os.path.join(\"Dataset\\\\Image130_resize\" ,'*jpg')): #130\n",
    "\timg = cv2.imread(i)\t\n",
    "\t#img = cv2.resize(img, (66,200), interpolation = cv2.INTER_AREA)\n",
    "\tX = np.append(X,img, axis = 0)\n",
    "print(\"Loading 70%..\")\n",
    "for i in glob.glob(os.path.join(\"Dataset\\\\Image140_resize\" ,'*jpg')): #140\n",
    "\timg = cv2.imread(i)\t\n",
    "\t#img = cv2.resize(img, (66,200), interpolation = cv2.INTER_AREA)\n",
    "\tX = np.append(X,img, axis = 0)\n",
    "for i in glob.glob(os.path.join(\"Dataset\\\\Image150_resize\" ,'*jpg')): #150\n",
    "\timg = cv2.imread(i)\t\n",
    "\t#img = cv2.resize(img, (66,200), interpolation = cv2.INTER_AREA)\n",
    "\tX = np.append(X,img, axis = 0)\n",
    "print(\"Loading 90%..\")\n",
    "for i in glob.glob(os.path.join(\"Dataset\\\\Image160_resize\" ,'*jpg')): #160\n",
    "\timg = cv2.imread(i)\t\n",
    "\t#img = cv2.resize(img, (66,200), interpolation = cv2.INTER_AREA)\n",
    "\tX = np.append(X,img, axis = 0)\n",
    "print(\"Finish\")\n",
    "X = X.reshape(-1,66,200,3)\n",
    "X.astype('float32')\n",
    "#y data\n",
    "y = np.asarray([0]*y1 + [1]*y2 + [2]*y3 + [3]*y4 + [4]*y5 + [5]*y6 + [6]*y7)\n",
    "y.astype('uint8')\n",
    "\n",
    "with open(\"data.txt\", \"wb\") as data:\n",
    "    pickle.dump((X,y), data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------from the second time onwards-----------------\n",
    "with open(\"data.txt\", \"rb\") as data:\n",
    "    X,y = pickle.load(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\tf13\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 30, 97, 8)         1184      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 13, 47, 16)        3216      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 5, 22, 32)         12832     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 3, 20, 64)         18496     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 1, 18, 64)         36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               115300    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 20)                1020      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 7)                 147       \n",
      "=================================================================\n",
      "Total params: 194,173\n",
      "Trainable params: 194,173\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(8, kernel_size=(7, 7),\t\t\t\t \n",
    "                 activation='relu',\n",
    "\t\t\t\t strides=(2, 2),\n",
    "                 input_shape= (66,200,3),\n",
    "\t\t\t\t padding='valid',))\n",
    "model.add(Conv2D(16, kernel_size=(5, 5), \n",
    "\t\t\t\t activation='relu',\n",
    "\t\t\t\t strides=(2, 2),\n",
    "\t\t\t\t padding='valid',))\n",
    "model.add(Conv2D(32, kernel_size=(5, 5), \n",
    "\t\t\t\t activation='relu',\n",
    "\t\t\t\t strides=(2, 2),\n",
    "\t\t\t\t padding='valid',))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), \n",
    "\t\t\t\t activation='relu',\n",
    "\t\t\t\t strides=(1, 1),\n",
    "\t\t\t\t padding='valid',))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), \n",
    "\t\t\t\t activation='relu',\n",
    "\t\t\t\t strides=(1, 1),\n",
    "\t\t\t\t padding='valid',))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(number_class, activation='softmax'))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split dataset : There are two methods to split data: Train/Test and K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting dataset follows as Train/Test method\n"
     ]
    }
   ],
   "source": [
    "#--------------------Train/Test-----------------------\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=ratio_testset, random_state=1)\n",
    "y_train = np_utils.to_categorical(y_train, number_class)\n",
    "y_valid = np_utils.to_categorical(y_valid, number_class)\n",
    "print(\"Splitting dataset follows as Train/Test method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting dataset follows as k-fold method\n"
     ]
    }
   ],
   "source": [
    "#--------------------K-Fold---------------------------\n",
    "y = np_utils.to_categorical(y, number_class)\n",
    "kfold = StratifiedShuffleSplit(n_splits=numbatchsplit, random_state=7,test_size=ratio_testset)\n",
    "print(\"Splitting dataset follows as k-fold method\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\tf13\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 16636 samples, validate on 2936 samples\n",
      "Epoch 1/10\n",
      " 4608/16636 [=======>......................] - ETA: 19s - loss: 1.3287 - acc: 0.4894"
     ]
    }
   ],
   "source": [
    "#--------------------Train with splitting data follows as Train/Test method------------\n",
    "checkpoint_path = os.path.join(logsdir, \"{epoch:04d}.h5\")\n",
    "tensorboard = [TensorBoard(log_dir=logsdir,\n",
    "\t\t\t\t\t\t\thistogram_freq=0, write_graph=True, write_images=False),\n",
    "\t\t\t\t\t\t\tkeras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "\t\t\t\t\t\t\tverbose=0, save_best_only=True),]\n",
    "Adam = keras.optimizers.Adam(lr=lrate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam, metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "\t\t\t\t\tbatch_size=batchsize,\n",
    "\t\t\t\t\tepochs=epochs,\n",
    "\t\t\t\t\tshuffle=True,\n",
    "\t\t\t\t\tvalidation_data=(X_valid, y_valid),\n",
    "\t\t\t\t\tcallbacks=tensorboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14749 samples, validate on 2603 samples\n",
      "Epoch 1/10\n",
      "14749/14749 [==============================] - 14s 938us/step - loss: 0.3385 - acc: 0.8630 - val_loss: 0.3439 - val_acc: 0.8636\n",
      "Epoch 2/10\n",
      " 5344/14749 [=========>....................] - ETA: 8s - loss: 0.3014 - acc: 0.8795"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-a3e41aa9e289>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m                         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m                         callbacks=tensorboard)\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\tf13\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\tf13\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\tf13\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\tf13\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\tf13\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#--------------------Train with splitting data follows as K-fold method------------\n",
    "Adam = keras.optimizers.Adam(lr=lrate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam, metrics=['accuracy'])\n",
    "\n",
    "i = 0\n",
    "for train_index, test_index in kfold.split(X,y):\n",
    "    checkpoint_path = os.path.join(logsdir,\"Model_fold_\" + str(i) + \"_Epoch_{epoch:04d}.h5\")\n",
    "    tensorboard = [TensorBoard(log_dir=logsdir,\n",
    " \t\t\t\t\thistogram_freq=0, write_graph=True, write_images=False),\n",
    "\t\t\t\t\tkeras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    " \t\t\t\t\tverbose=0, save_best_only=True),]\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    model.fit(X_train, y_train,\n",
    "                        batch_size=batchsize,\n",
    "                        epochs=epochs,\n",
    "                        shuffle=True,\n",
    "                        validation_data=(X_test, y_test),\n",
    "                        callbacks=tensorboard)\n",
    "    i = i +1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
